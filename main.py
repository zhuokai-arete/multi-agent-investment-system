from data_utils.data_loader import load_market_data
from agents.RiskParityAgent import RiskParityAgent
from agents.CVaRDownsideVolAgent import CVaRDownsideVolAgent
from agents.DRLAgent import PPOAgentContinuous
from controllers.debate_room import DebateRoomAgent
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# åŠ è½½æ•°æ®
print("âœ… Main: å¼€å§‹åŠ è½½æ•°æ®")
returns, available_assets, rolling_cov_matrices, benchmark, feature_data, asset_list = load_market_data("èµ„äº§æ± .xlsx", "ä¸­è¯å¤šèµ„äº§é£é™©å¹³ä»·æŒ‡æ•°.xlsx")

print("âœ… Main: åˆå§‹åŒ– RiskParityAgent")
risk_agent = RiskParityAgent(returns, available_assets, rolling_cov_matrices)
print("âœ… Main: åˆå§‹åŒ– CVaRDownsideVolAgent")
cvar_agent = CVaRDownsideVolAgent(returns, available_assets, risk_agent.get_full_weights(), rolling_cov_matrices)
state_dim = feature_data.shape[1] + 2 * len(asset_list)
action_dim = len(asset_list)
print("âœ… Main: åˆå§‹åŒ– PPOAgentContinuous")
drl_agent = PPOAgentContinuous(state_dim=state_dim, action_dim=action_dim, action_asset_list=asset_list)

train_end_date, test_start_date = '2020-12-31', '2021-01-04'
train_dates = returns.index[returns.index <= train_end_date]
test_dates = returns.index[returns.index >= test_start_date]
train_features, test_features = feature_data.loc[train_dates], feature_data.loc[test_dates]

# æ„é€ è®­ç»ƒæ•°æ®
states, actions, rewards, masks = [], [], [], []
portfolio_value_prev = 1.0
for date in train_dates:
    market_state = {'date': date}
    feature_vector = feature_data.loc[date].values.tolist()
    rp_decision = risk_agent.get_decision(market_state).reindex(returns.columns, fill_value=0)
    cvar_decision = cvar_agent.get_decision(market_state).reindex(returns.columns, fill_value=0)

    ppo_input = np.concatenate([feature_vector, rp_decision.values, cvar_decision.values])
    ppo_return = np.sum(returns.loc[date].fillna(0) * rp_decision)  # è¿™é‡Œä»ä½¿ç”¨ rp_return æ¨¡æ‹Ÿ
    volatility = np.std(returns.loc[date].fillna(0))
    cost = 0.001
    rule_return = np.sum(returns.loc[date].fillna(0) * ((rp_decision + cvar_decision) / 2))
    alpha, lambd = 0.1, 0.05
    portfolio_value_now = portfolio_value_prev * (1 + ppo_return)
    reward = np.log(portfolio_value_now / portfolio_value_prev) - lambd * volatility - cost + alpha * (ppo_return - rule_return)
    portfolio_value_prev = portfolio_value_now

    # âœ… ä½¿ç”¨å¹³å‡ç­–ç•¥ä½œä¸ºè®­ç»ƒåŠ¨ä½œï¼ˆè¿ç»­å‘é‡ï¼‰
    combined_decision = ((rp_decision + cvar_decision) / 2).values
    states.append(ppo_input)
    actions.append(combined_decision)
    rewards.append(reward)
    masks.append(1)

# âœ… æ„å»ºè¿ç»­ç­–ç•¥ä¸‹çš„è®­ç»ƒæ•°æ®ï¼ˆå››å…ƒç»„ï¼‰
trajectories = list(zip(states, actions, rewards, masks))
print("ğŸš€ å¼€å§‹è®­ç»ƒ PPOAgentContinuous")
drl_agent.train(trajectories)


def run_evaluation(dates, features):
    daily_returns = []
    for date in dates:
        feature_vector = features.loc[date].values
        rp_decision = risk_agent.get_decision({'date': date}).reindex(returns.columns, fill_value=0).values
        cvar_decision = cvar_agent.get_decision({'date': date}).reindex(returns.columns, fill_value=0).values
        market_state_full = {
            'feature_vector': feature_vector,
            'rp_decision': rp_decision,
            'cvar_decision': cvar_decision,
            'date': date
        }
        decision = drl_agent.get_decision(market_state_full)  # é‡‡ç”¨æ–¹æ³•1
        decision = decision.reindex(returns.columns, fill_value=0)
        daily_return = np.sum(returns.loc[date].fillna(0) * decision)
        daily_returns.append(daily_return)
    return daily_returns


def calculate_metrics(daily_returns):
    daily_returns = np.array(daily_returns)
    annual_return = np.mean(daily_returns) * 252
    annual_volatility = np.std(daily_returns) * np.sqrt(252)
    sharpe_ratio = annual_return / annual_volatility if annual_volatility != 0 else 0
    cumulative = np.cumprod(1 + daily_returns)
    max_drawdown = np.max(np.maximum.accumulate(cumulative) - cumulative) / np.max(np.maximum.accumulate(cumulative))
    return annual_return, annual_volatility, sharpe_ratio, max_drawdown

train_returns = run_evaluation(train_dates, train_features)
test_returns = run_evaluation(test_dates, test_features)

train_metrics = calculate_metrics(train_returns)
test_metrics = calculate_metrics(test_returns)
print("ğŸ“ˆ è®­ç»ƒé›†ï¼šå¹´åŒ–æ”¶ç›Š {:.2%}, å¹´åŒ–æ³¢åŠ¨ {:.2%}, å¤æ™®æ¯”ç‡ {:.2f}, æœ€å¤§å›æ’¤ {:.2%}".format(*train_metrics))
print("ğŸ“ˆ æµ‹è¯•é›†ï¼šå¹´åŒ–æ”¶ç›Š {:.2%}, å¹´åŒ–æ³¢åŠ¨ {:.2%}, å¤æ™®æ¯”ç‡ {:.2f}, æœ€å¤§å›æ’¤ {:.2%}".format(*test_metrics))

print("âœ… Main: åˆå§‹åŒ– DebateRoomAgent")
controller = DebateRoomAgent([risk_agent, cvar_agent, drl_agent])
rp_returns, cvar_returns, drl_returns, debate_returns, dates_list = [], [], [], [], []
for date in returns.index:
    if date not in available_assets: continue
    market_state = {'date': date}
    daily_ret = returns.loc[date].fillna(0)
    rp_decision = risk_agent.get_decision(market_state).reindex(returns.columns, fill_value=0)
    cvar_decision = cvar_agent.get_decision(market_state).reindex(returns.columns, fill_value=0)
    ppo_input = np.concatenate([feature_data.loc[date].values, rp_decision.values, cvar_decision.values])
    drl_decision = pd.Series(drl_agent.get_action(ppo_input),index=drl_agent.action_asset_list).reindex(returns.columns, fill_value=0)

    feature_vector = feature_data.loc[date].values
    rp_decision = risk_agent.get_decision(market_state).reindex(returns.columns, fill_value=0).values
    cvar_decision = cvar_agent.get_decision(market_state).reindex(returns.columns, fill_value=0).values
    market_state_full = {
        'feature_vector': feature_vector,
        'rp_decision': rp_decision,
        'cvar_decision': cvar_decision,
        'date': date
    }
    debate_decision = controller.get_decision(market_state_full).reindex(returns.columns, fill_value=0)

    
    rp_returns.append(np.sum(daily_ret * rp_decision))
    cvar_returns.append(np.sum(daily_ret * cvar_decision))
    drl_returns.append(np.sum(daily_ret * drl_decision))
    debate_returns.append(np.sum(daily_ret * debate_decision))
    dates_list.append(date)

metrics_list = [
    ('RiskParity', calculate_metrics(rp_returns)),
    ('CVaR', calculate_metrics(cvar_returns)),
    ('PPO', calculate_metrics(drl_returns)),
    ('DebateRoom', calculate_metrics(debate_returns))
]
for name, metrics in metrics_list:
    print(f"ğŸ“ˆ {name} : å¹´åŒ–æ”¶ç›Š {metrics[0]:.2%}, å¹´åŒ–æ³¢åŠ¨ {metrics[1]:.2%}, å¤æ™®æ¯”ç‡ {metrics[2]:.2f}, æœ€å¤§å›æ’¤ {metrics[3]:.2%}")

plt.figure(figsize=(12, 6))
cumulative_curves = {
    'RiskParity': np.cumprod(1 + np.array(rp_returns)),
    'CVaR': np.cumprod(1 + np.array(cvar_returns)),
    'PPO': np.cumprod(1 + np.array(drl_returns)),
    'DebateRoom': np.cumprod(1 + np.array(debate_returns))
}
for label, curve in cumulative_curves.items():
    plt.plot(dates_list, curve, label=label)
plt.xlabel('Date')
plt.ylabel('Cumulative Net Value')
plt.title('Cumulative Performance Comparison')
plt.legend()
plt.grid(True)
plt.savefig('agent_performance_comparison.png')
plt.show()
print("âœ… æ‰€æœ‰ç»“æœè¾“å‡ºä¸å›¾å½¢ç»˜åˆ¶å®Œæ¯•")



